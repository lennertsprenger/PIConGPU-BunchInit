{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb61b395-541f-4985-81fc-96d60b1fbf45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import openpmd_api as io\n",
    "import numpy as np\n",
    "from numba import cuda, jit\n",
    "import math\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import scipy.constants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab53ac8a-a684-4707-b9d0-f524b2697b17",
   "metadata": {},
   "source": [
    "## Things to do in the future\n",
    "\n",
    "* specify a cutoff, behind all fields are written as 0 instead of a small value e.g. 1e-10 * field_max\n",
    "* multi GPU / multi Node calculations  field size are limited by GPU memory size\n",
    "\n",
    "## Things to note\n",
    "* The Convolutional PML fields aren't changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5116cf15-2267-4ec1-a49e-ba36acd56324",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this cell is only required for testing the write proccess\n",
    "try:\n",
    "    del readSeries\n",
    "    print(\"del readSeries\")\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    del writeSeries\n",
    "    print(\"del writeSereies\")\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fa483f-bb3e-4cbd-9c59-609e82aed735",
   "metadata": {},
   "source": [
    "## Set path to checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6646086f-be87-4014-b985-bbca20213b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# you can use one checkpoint explicitly checkpoint_0.h5\n",
    "# or a time series with checkpoint_%T.h5\n",
    "# the warning can be ignored when using the explicit checkpoint, we want just a single file\n",
    "#path = \"/bigdata/hplsim/scratch/spreng88/runs/bigFieldWithoutInit23/simOutput/checkpoints/checkpoint_0.h5\"\n",
    "readFile = \"/bigdata/hplsim/scratch/spreng88/runs/old/restart/tmp_%T.h5\"\n",
    "outputFile = \"/bigdata/hplsim/scratch/spreng88/runs/restart/write_%T.h5\"\n",
    "\n",
    "#readFile = outputFile\n",
    "# io.Access.read_write only works with h5 files\n",
    "readSeries = io.Series(readFile, io.Access.read_only)\n",
    "writeSeries = io.Series(outputFile, io.Access.create)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d917d7d8-d715-401b-b0fb-f3eece853c1a",
   "metadata": {},
   "source": [
    "## General setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca29448d-bcba-4506-8b13-4b9914ef8306",
   "metadata": {},
   "outputs": [],
   "source": [
    "nth = 1             # only use every nth - particle (for faster calculation for tests)\n",
    "\n",
    "do_write = False       # True if the calculated field's should be written directly after calculations\n",
    "                    # False if you want to look at the fields before writing them\n",
    "\n",
    "# add field to iteration 0\n",
    "iteration = readSeries.iterations[0] \n",
    "\n",
    "# Read attributes of the simulation that might be needed later\n",
    "# Everything is calculated in PIConGPU units\n",
    "# For SI units: length_si = value_pic * unit_length\n",
    "cell_depth = iteration.get_attribute(\"cell_depth\")          # z\n",
    "cell_height = iteration.get_attribute(\"cell_height\")        # y\n",
    "cell_width = iteration.get_attribute(\"cell_width\")          # x\n",
    "\n",
    "unit_efield = iteration.get_attribute(\"unit_efield\")\n",
    "unit_bfield = iteration.get_attribute(\"unit_bfield\")\n",
    "unit_charge = iteration.get_attribute(\"unit_charge\")\n",
    "unit_mass = iteration.get_attribute(\"unit_mass\")\n",
    "unit_speed = iteration.get_attribute(\"unit_speed\")\n",
    "unit_length = iteration.get_attribute(\"unit_length\")\n",
    "unit_time = iteration.get_attribute(\"unit_time\")\n",
    "\n",
    "pi = scipy.constants.pi\n",
    "c = scipy.constants.c / unit_speed\n",
    "eps0 = iteration.get_attribute(\"eps0\")\n",
    "mue0 = iteration.get_attribute(\"mue0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba1a52d-92ea-4f84-8966-389366bb9fa4",
   "metadata": {},
   "source": [
    "## Cuda functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db0ff10-91d1-4267-b1e9-c9d3ebfba29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@cuda.jit(device=True)\n",
    "def particleV(px, py, pz, mass):\n",
    "    \"\"\"\n",
    "    calculate particle speed from momentum\n",
    "    \"\"\"\n",
    "    \n",
    "    m2p2 = math.sqrt( (mass)**2 + px**2 + py**2 + pz**2)\n",
    "    \n",
    "    vx = px / m2p2 * c\n",
    "    vy = py / m2p2 * c\n",
    "    vz = pz / m2p2 * c\n",
    "    \n",
    "    return vx, vy, vz\n",
    "\n",
    "\n",
    "@cuda.jit(device=True)\n",
    "def param1(rx,ry,rz, rqx,rqy,rqz, vx,vy,vz):\n",
    "    \"\"\"\n",
    "    calculate some parameters that could be reused\n",
    "    r: position of field calculation\n",
    "    rq: position of charge q at time t\n",
    "    rq_tr: position of charge at retarded time tr\n",
    "    \"\"\"\n",
    "    \n",
    "    #\n",
    "    # solution equation:\n",
    "    # dt = t - tr   = time - retarded_time\n",
    "    # r where we want the field\n",
    "    # rq position of charge\n",
    "    # |c*dt| = |r - (rq - v*dt)|\n",
    "    # solve for dt:\n",
    "    # => (c^2 - |v|^2) * dt^2 - 2(r*v - rq*v) * dt + 2 * r*rq - |r|^2 - |rq|^2\n",
    "    #\n",
    "    a = (c**2 - (vx**2 + vy**2 + vz**2))\n",
    "    b = -2 * ((rx-rqx)*vx + (ry-rqy)*vy + (rz-rqz)*vz)\n",
    "    d = 2 * (rx*rqx + ry*rqy + rz*rqz) - (rx**2 + ry**2 + rz**2) - (rqx**2 + rqy**2 + rqz**2)\n",
    "    dt = (-b + math.sqrt(b**2 - 4*a*d)) / (2*a)\n",
    "    \n",
    "    rq_trx = rqx - vx * dt\n",
    "    rq_try = rqy - vy * dt\n",
    "    rq_trz = rqz - vz * dt\n",
    "\n",
    "    dvx = rx - rq_trx\n",
    "    dvy = ry - rq_try\n",
    "    dvz = rz - rq_trz\n",
    "    \n",
    "    distance = math.sqrt(dvx**2 + dvy**2 + dvz**2)\n",
    "    \n",
    "    if distance == 0:\n",
    "        distance = -1\n",
    "    \n",
    "    #n =  distanceVec / distance\n",
    "    nx = dvx / distance\n",
    "    ny = dvy / distance\n",
    "    nz = dvz / distance\n",
    "    \n",
    "    return nx, ny, nz, distance, dvx, dvy, dvz #n, distance, distanceVec\n",
    "\n",
    "\n",
    "@cuda.jit(device=True)\n",
    "def retardedEFieldParallel(q, nx, ny, nz, distance, dvx, dvy, dvz, vx, vy, vz):\n",
    "    \"\"\"\n",
    "    field for one position\n",
    "    q: charge\n",
    "    nx, ny, nz: unit vector to retarded position\n",
    "    distance: distance to retarded position\n",
    "    dvx, dvy, dvz: distance Vector to retarded position\n",
    "    vx, vy, vz: velocity vector of the charge\n",
    "    \"\"\"\n",
    "    \n",
    "    if distance == -1:\n",
    "        nx = 1\n",
    "        ny = 0\n",
    "        xz = 0\n",
    "    \n",
    "    factor = q / (4 * pi * eps0)\n",
    "    \n",
    "    ux = c * nx - vx\n",
    "    uy = c * ny - vy\n",
    "    uz = c * nz - vz\n",
    "    \n",
    "\n",
    "    scalar = factor * distance / (dvx*ux + dvy*uy + dvz*uz)**3 * (c**2 - (vx**2 + vy**2 + vz**2))\n",
    "    \n",
    "    return  scalar * ux, scalar * uy, scalar * uz\n",
    "\n",
    "\n",
    "@cuda.jit(device=True)\n",
    "def returnFields(q, rx,rex,ry,rey,rz,rez, rqx,rqy,rqz, px,py,pz, mass):\n",
    "    \"\"\"calculate the fields for the position,\n",
    "    currently ignoring the offset between the grid corner and field points\n",
    "    because we will use smoothing in the end over the field anyway\"\"\"\n",
    "    \n",
    "    vx, vy, vz = particleV(px, py, pz, mass)\n",
    "    \n",
    "    ## E Field ------------------------------------------------------------\n",
    "    nx, ny, nz, distancez, dvx, dvy, dvz = param1(rx, ry, rz, rqx, rqy, rqz, vx,vy,vz)\n",
    "    ex, ey, ez = retardedEFieldParallel(q, nx, ny, nz, distancez, dvx, dvy, dvz , vx, vy, vz)\n",
    "\n",
    "    \n",
    "    ## B Field ------------------------------------------------------------\n",
    "    bx = ( ny * ez - nz * ey ) / c\n",
    "    by = ( nz * ex - nx * ez ) / c\n",
    "    bz = ( nx * ey - ny * ex ) / c\n",
    "    \n",
    "    return ex, ey, ez, bx, by, bz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768327f3-e5ba-4e7a-b2d4-536cafb14301",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def particleParallel(Ex,Ey,Ez, Bx,By,Bz, q_, rqx_,rqy_,rqz_, px_,py_,pz_, mass_, weighting_, xdim, ydim, zdim, particleCount):\n",
    "    \"\"\"\n",
    "    Ex,Ey,Ez E field where new field is added\n",
    "    Bx,By,Bz B -\"-\n",
    "    \n",
    "    _ is on all input arrays read from checkpoint \n",
    "    q_ array from checkpoint with charges\n",
    "    rq_ position data of particles\n",
    "    p_ momentum of particles\n",
    "    weighting_ macro particle weighting\n",
    "    \n",
    "    xdim, ydim, zdim length of dimension of Ex,Bx, ...\n",
    "    Ex/Bx need to be 1D for atomic add\n",
    "    \"\"\"\n",
    "    \n",
    "    tix = cuda.threadIdx.x\n",
    "    bix = cuda.blockIdx.x\n",
    "    bdx = cuda.blockDim.x\n",
    "    \n",
    "    index = tix + bix * bdx    # particle index\n",
    "    index = index\n",
    "    \n",
    "    if index < particleCount:\n",
    "    \n",
    "        q = q_[index] * weighting_[index]\n",
    "        rqx = rqx_[index] * cell_width\n",
    "        rqy = rqy_[index] * cell_height\n",
    "        rqz = rqz_[index] * cell_depth\n",
    "        px = px_[index]\n",
    "        py = py_[index]\n",
    "        pz = pz_[index]\n",
    "        mass = mass_[index] * weighting_[index]\n",
    "\n",
    "        for x in range(xdim):\n",
    "\n",
    "            rx = x * cell_width\n",
    "            rex = (x + 0.5) * cell_width\n",
    "\n",
    "            for y in range(ydim):\n",
    "\n",
    "                ry = y * cell_height\n",
    "                rey = (y + 0.5) * cell_height\n",
    "\n",
    "                for z in range(zdim):    \n",
    "                    \n",
    "                    rz = z * cell_depth\n",
    "                    rez = (z + 0.5) * cell_depth\n",
    "\n",
    "                    fieldIndex = x + y * xdim + z * xdim * ydim\n",
    "                    \n",
    "                    ex, ey, ez, bx, by, bz = returnFields(q, rx,rex,ry,rey,rz,rez, rqx,rqy,rqz, px,py,pz, mass)\n",
    "                    \n",
    "                    cuda.atomic.add(Ex, fieldIndex, ex * nth)\n",
    "                    cuda.atomic.add(Ey, fieldIndex, ey * nth)\n",
    "                    cuda.atomic.add(Ez, fieldIndex, ez * nth)\n",
    "\n",
    "                    cuda.atomic.add(Bx, fieldIndex, bx)\n",
    "                    cuda.atomic.add(By, fieldIndex, by)\n",
    "                    cuda.atomic.add(Bz, fieldIndex, bz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3087f5-be7e-4461-b028-2f148653a3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def FieldParallel(Ex,Ey,Ez, Bx,By,Bz, q_, rqx_,rqy_,rqz_, px_,py_,pz_, mass_, weighting_, xdim, ydim, zdim, particleCount):\n",
    "    \"\"\"\n",
    "    Ex,Ey,Ez E field where new field is added\n",
    "    Bx,By,Bz B -\"-\n",
    "    \n",
    "    _ is on all input arrays read from checkpoint \n",
    "    q_ array from checkpoint with charges\n",
    "    rq_ position data of particles\n",
    "    p_ momentum of particles\n",
    "    \n",
    "    xdim, ydim, zdim length of dimension of Ex,Bx, ...\n",
    "    Ex/Bx need to be 1D for atomic add\n",
    "    \"\"\"\n",
    "    tix = cuda.threadIdx.x\n",
    "    bix = cuda.blockIdx.x\n",
    "    bdx = cuda.blockDim.x\n",
    "    \n",
    "    fieldIndex = tix + bix * bdx    # field index\n",
    "    \n",
    "    # calculate 3d coordinate from 1d field coordinate array\n",
    "    # fieldIndex = x + y * xdim + z * xdim * ydim\n",
    "    z = math.floor( fieldIndex / (xdim*ydim) )\n",
    "    y = math.floor( (fieldIndex - z * xdim*ydim) / xdim )\n",
    "    x = fieldIndex - z * xdim*ydim - y * xdim\n",
    "    \n",
    "    if fieldIndex < xdim*ydim*zdim:\n",
    "        \n",
    "        #for index in range(particleCount):\n",
    "        # or add filter if you only want the field of some particles of that species\n",
    "        for index in range(particleCount):\n",
    "        \n",
    "            q = q_[index] * weighting_[index]\n",
    "            rqx = rqx_[index] * cell_width\n",
    "            rqy = rqy_[index] * cell_height\n",
    "            rqz = rqz_[index] * cell_depth\n",
    "            px = px_[index]\n",
    "            py = py_[index]\n",
    "            pz = pz_[index]\n",
    "            mass = mass_[index] * weighting_[index]\n",
    "            \n",
    "            rx = x * cell_width\n",
    "            rex = (x + 0.5) * cell_width\n",
    "            \n",
    "            ry = y * cell_height\n",
    "            rey = (y + 0.5) * cell_height\n",
    "            \n",
    "            rz = z * cell_depth\n",
    "            rez = (z + 0.5) * cell_depth\n",
    "            \n",
    "            ex, ey, ez, bx, by, bz = returnFields(q, rx,rex,ry,rey,rz,rez, rqx,rqy,rqz, px,py,pz, mass)\n",
    "            if((x == 3)and(z%200==0)):\n",
    "                pass\n",
    "                #print(rqx,rqy,rqz)\n",
    "\n",
    "            cuda.atomic.add(Ex, fieldIndex, ex * nth)\n",
    "            cuda.atomic.add(Ey, fieldIndex, ey * nth)\n",
    "            cuda.atomic.add(Ez, fieldIndex, ez * nth)\n",
    "\n",
    "            cuda.atomic.add(Bx, fieldIndex, bx)\n",
    "            cuda.atomic.add(By, fieldIndex, by)\n",
    "            cuda.atomic.add(Bz, fieldIndex, bz)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78dbe31-11bd-4fa7-a0a7-f7e878713505",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_attributes(inputAtr, outputAtr):\n",
    "    for attribute in inputAtr.attributes:\n",
    "        #print(\"attribute copied:\",attribute)\n",
    "        attribute_value = inputAtr.get_attribute(attribute)\n",
    "        outputAtr.set_attribute(attribute, attribute_value)\n",
    "        \n",
    "def copy_series_attributes(inputAtr, outputAtr):\n",
    "    attributes_to_skip = ['openPMD', 'openPMDextension']\n",
    "    for attribute in inputAtr.attributes:\n",
    "        if attribute in attributes_to_skip:\n",
    "            continue\n",
    "        #print(\"attribute copied:\",attribute)\n",
    "        attribute_value = inputAtr.get_attribute(attribute)\n",
    "        outputAtr.set_attribute(attribute, attribute_value)\n",
    "        \n",
    "def print_attributes(atr):\n",
    "    try:\n",
    "        for attribute in atr.attributes:\n",
    "            print(attribute, atr.get_attribute(attribute))\n",
    "        print()\n",
    "    except:\n",
    "        print('errro in print attributes')\n",
    "\n",
    "\n",
    "def copy_particle_data(inputIteration, outputIteration):\n",
    "    # copy the all particle data from input to output iteration\n",
    "\n",
    "    global writeSeries\n",
    "    \n",
    "    for species in inputIteration.particles:\n",
    "        copy_attributes(inputIteration.particles[species], outputIteration.particles[species])\n",
    "        \n",
    "        for species_data in inputIteration.particles[species]:\n",
    "            \n",
    "            copy_attributes(inputIteration.particles[species][species_data], outputIteration.particles[species][species_data])\n",
    "\n",
    "            if inputIteration.particles[species][species_data].scalar:\n",
    "                \n",
    "                component_data = inputIteration.particles[species][species_data][io.Record_Component.SCALAR][:]\n",
    "                readSeries.flush()\n",
    "                output_component_data = outputIteration.particles[species][species_data]\n",
    "                \n",
    "                if 'value' in inputIteration.particles[species][species_data].attributes:\n",
    "                    value = inputIteration.particles[species][species_data].get_attribute('value')\n",
    "                    output_component_data[io.Record_Component.SCALAR].make_constant(value)\n",
    "                \n",
    "                else:\n",
    "                    dataset = io.Dataset( component_data.dtype, component_data.shape )\n",
    "                    output_component_data[io.Record_Component.SCALAR].reset_dataset(dataset)\n",
    "                    output_component_data[io.Record_Component.SCALAR][()] = component_data\n",
    "\n",
    "            else:\n",
    "                output_species_component = outputIteration.particles[species][species_data]\n",
    "                for component in inputIteration.particles[species][species_data]:\n",
    "                    #component is 'x', 'y' or 'z'\n",
    "                    \n",
    "                    output_component_data = output_species_component[component]\n",
    "                    copy_attributes(inputIteration.particles[species][species_data][component], output_component_data)\n",
    "                    \n",
    "                    component_data = inputIteration.particles[species][species_data][component][:]\n",
    "                    readSeries.flush()\n",
    "                    \n",
    "                    dataset = io.Dataset( component_data.dtype, component_data.shape )\n",
    "                    output_component_data.reset_dataset(dataset)\n",
    "                    \n",
    "                    output_component_data.store_chunk(component_data)\n",
    "            writeSeries.flush()\n",
    "\n",
    "            \n",
    "        for patch_component in inputIteration.particles[species].particle_patches:\n",
    "            #print(patch_component)\n",
    "            if inputIteration.particles[species].particle_patches[patch_component].scalar:\n",
    "                output_component_data = outputIteration.particles[species].particle_patches[patch_component]\n",
    "                # read-only !\n",
    "                copy_attributes(inputIteration.particles[species].particle_patches[patch_component], output_component_data)\n",
    "\n",
    "                component_data = inputIteration.particles[species].particle_patches[patch_component][io.Record_Component.SCALAR].load()\n",
    "                readSeries.flush()\n",
    "                #print(component_data, component_data.dtype, component_data.shape)\n",
    "\n",
    "                dataset = io.Dataset( component_data.dtype, component_data.shape )\n",
    "                output_component_data[io.Record_Component.SCALAR].reset_dataset(dataset)\n",
    "                #outputIteration.particles[species].particle_patches[patch_component][io.Record_Component.SCALAR].reset_dataset(dataset)\n",
    "\n",
    "                for i in range(len(component_data)):\n",
    "                    output_component_data[io.Record_Component.SCALAR].store(i, component_data[i])\n",
    "                    #outputIteration.particles[species].particle_patches[patch_component][io.Record_Component.SCALAR].store(i, component_data[i])\n",
    "\n",
    "            else:\n",
    "                copy_attributes(inputIteration.particles[species].particle_patches[patch_component], outputIteration.particles[species].particle_patches[patch_component])\n",
    "\n",
    "                output_species_component = outputIteration.particles[species].particle_patches[patch_component]\n",
    "                for component in inputIteration.particles[species].particle_patches[patch_component]:\n",
    "                    #component is 'x', 'y' or 'z'\n",
    "                    #print(component)\n",
    "                    output_component_data = output_species_component[component]\n",
    "                    copy_attributes(inputIteration.particles[species].particle_patches[patch_component][component], output_component_data)\n",
    "\n",
    "                    component_data = inputIteration.particles[species].particle_patches[patch_component][component].load()\n",
    "                    readSeries.flush()\n",
    "\n",
    "                    dataset = io.Dataset( component_data.dtype, component_data.shape )\n",
    "                    output_component_data.reset_dataset(dataset)\n",
    "\n",
    "                    for i in range(len(component_data)):\n",
    "                        output_component_data.store(i, component_data[i])\n",
    "        \n",
    "        # can't write particles patches before all data is set\n",
    "        writeSeries.flush()\n",
    "        \n",
    "        \n",
    "def copy_field_data(inputIteration, outputIteration):\n",
    "    copy_attributes(inputIteration.meshes, outputIteration.meshes)\n",
    "\n",
    "    for field_name in iteration.meshes:\n",
    "\n",
    "        if field_name in ['E', 'B']:\n",
    "            #print(field_name)\n",
    "            copy_attributes(inputIteration.meshes[field_name], outputIteration.meshes[field_name])\n",
    "            \n",
    "            for component in inputIteration.meshes[field_name]:\n",
    "                #print(\"     \", component)\n",
    "                copy_attributes(inputIteration.meshes[field_name][component], outputIteration.meshes[field_name][component])\n",
    "                \n",
    "                dataset = io.Dataset( ex.dtype, ex.reshape(zdim, ydim, xdim).shape )    # or ey, ez, bx, by or bz instead\n",
    "                \n",
    "                if field_name == 'E':\n",
    "                    if component == 'x':\n",
    "                        data = bx\n",
    "                    elif component == 'y':\n",
    "                        data = by\n",
    "                    elif component == 'z':\n",
    "                        data = bz\n",
    "                        \n",
    "                elif field_name == 'B':\n",
    "                    if component == 'x':\n",
    "                        data = bx\n",
    "                    elif component == 'y':\n",
    "                        data = by\n",
    "                    elif component == 'z':\n",
    "                        data = bz\n",
    "                \n",
    "                outputIteration.meshes[field_name][component].reset_dataset(dataset)\n",
    "                outputIteration.meshes[field_name][component].store_chunk(data.reshape(zdim, ydim, xdim))\n",
    "                writeSeries.flush()\n",
    "            continue\n",
    "\n",
    "        #print(field_name)\n",
    "        field = outputIteration.meshes[field_name]\n",
    "        copy_attributes(iteration.meshes[field_name], field)\n",
    "\n",
    "        if field_name == 'picongpu_idProvider':\n",
    "            for component in inputIteration.meshes[field_name]:\n",
    "\n",
    "                copy_attributes(inputIteration.meshes[field_name][component], field[component])\n",
    "\n",
    "                data = inputIteration.meshes[field_name][component].load_chunk()\n",
    "\n",
    "                dataset = io.Dataset(data.dtype, data.shape)\n",
    "                field[component].reset_dataset(dataset)\n",
    "\n",
    "                field[component].store_chunk(data)\n",
    "            continue\n",
    "\n",
    "        if inputIteration.meshes[field_name].scalar:\n",
    "            data = inputIteration.meshes[field_name][io.Record_Component.SCALAR].load_chunk()\n",
    "            readSeries.flush()\n",
    "\n",
    "            dataset = io.Dataset( data.dtype, data.shape)\n",
    "            field[io.Record_Component.SCALAR].reset_dataset(dataset)\n",
    "\n",
    "            field[io.Record_Component.SCALAR].store_chunk()\n",
    "\n",
    "        else:\n",
    "\n",
    "            for component in inputIteration.meshes[field_name]:\n",
    "                #print(\"   \",component)\n",
    "\n",
    "                data = inputIteration.meshes[field_name][component].load_chunk()\n",
    "                readSeries.flush()\n",
    "                dataset = io.Dataset(data.dtype, data.shape)\n",
    "\n",
    "                output_data = outputIteration.meshes[field_name][component]\n",
    "                output_data.reset_dataset(dataset)\n",
    "                output_data.store_chunk(data)\n",
    "\n",
    "        writeSeries.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56c7b84-318a-4dad-b75f-d0b740d20f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write():\n",
    "    \n",
    "    global writeSeries\n",
    "    \n",
    "    dim = ['x', 'y', 'z']\n",
    "\n",
    "    # copy general attributes ----------------------------------\n",
    "    copy_series_attributes(readSeries, writeSeries)\n",
    "\n",
    "    outputIteration = writeSeries.iterations[0]\n",
    "\n",
    "    copy_attributes(iteration, outputIteration)\n",
    "    \n",
    "    # copy particle data\n",
    "    copy_particle_data(iteration, outputIteration)\n",
    "    \n",
    "    # copy/write field data\n",
    "    copy_field_data(iteration, outputIteration)\n",
    "    \n",
    "    del writeSeries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8a1a12-c07c-4882-8b3a-61c7c4dc2f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothFields(ex, ey, ez, bx, by, bz, sigma=4):\n",
    "    ex = gaussian_filter(ex.reshape(zdim, ydim, xdim), sigma)\n",
    "    ey = gaussian_filter(ey.reshape(zdim, ydim, xdim), sigma)\n",
    "    ez = gaussian_filter(ez.reshape(zdim, ydim, xdim), sigma)\n",
    "    bx = gaussian_filter(bx.reshape(zdim, ydim, xdim), sigma)\n",
    "    by = gaussian_filter(by.reshape(zdim, ydim, xdim), sigma)\n",
    "    bz = gaussian_filter(bz.reshape(zdim, ydim, xdim), sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17328d3e-644d-4182-b9d8-12bbdd87c016",
   "metadata": {},
   "source": [
    "## load particle data from the checkpoint\n",
    "define the species identifier string, as in the simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7952feba-32f4-4fab-a5c5-d7dd29dee239",
   "metadata": {},
   "outputs": [],
   "source": [
    "species = \"b\"\n",
    "\n",
    "xpos_incell = iteration.particles[species][\"position\"][\"x\"][:]\n",
    "ypos_incell = iteration.particles[species][\"position\"][\"y\"][:]\n",
    "zpos_incell = iteration.particles[species][\"position\"][\"z\"][:]\n",
    "xpos_offset = iteration.particles[species][\"positionOffset\"][\"x\"][:]\n",
    "ypos_offset = iteration.particles[species][\"positionOffset\"][\"y\"][:]\n",
    "zpos_offset = iteration.particles[species][\"positionOffset\"][\"z\"][:]\n",
    "momentumx = iteration.particles[species][\"momentum\"][\"x\"][:]\n",
    "momentumy = iteration.particles[species][\"momentum\"][\"y\"][:]\n",
    "momentumz = iteration.particles[species][\"momentum\"][\"z\"][:]\n",
    "weightings = iteration.particles[species][\"weighting\"][io.Record_Component.SCALAR][:]\n",
    "charge = iteration.particles[species][\"charge\"][io.Record_Component.SCALAR][:]\n",
    "mass = iteration.particles[species][\"mass\"][io.Record_Component.SCALAR][:]\n",
    "\n",
    "readSeries.flush()\n",
    "\n",
    "xpos = xpos_incell + np.float32(xpos_offset)\n",
    "ypos = ypos_incell + np.float32(ypos_offset)\n",
    "zpos = zpos_incell + np.float32(zpos_offset)\n",
    "\n",
    "# free some memory\n",
    "del xpos_incell, ypos_incell, zpos_incell\n",
    "del xpos_offset, ypos_offset, zpos_offset\n",
    "\n",
    "\n",
    "# use just every nth particle\n",
    "# needs to be in a new numpy array\n",
    "# because it must be continous in memory to be copied to gpu\n",
    "momentumx = np.array(momentumx[::nth])\n",
    "momentumy = np.array(momentumy[::nth])\n",
    "momentumz = np.array(momentumz[::nth])\n",
    "weightings = np.array(weightings[::nth])\n",
    "charge = np.array(charge[::nth])\n",
    "mass = np.array(mass[::nth])\n",
    "xpos = np.array(xpos[::nth])\n",
    "ypos = np.array(ypos[::nth])\n",
    "zpos = np.array(zpos[::nth])\n",
    "particleCount = len(mass)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80117bf6-b892-40ef-8f3d-5ee74a748a4f",
   "metadata": {},
   "source": [
    "## output to check particleCount and gridsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba100d4d-b270-4ee3-9aa9-72fe0e089514",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"particleCount:\", particleCount)\n",
    "fieldShape = iteration.meshes[\"E\"][\"x\"][:].shape\n",
    "xdim = fieldShape[2]\n",
    "ydim = fieldShape[1]\n",
    "zdim = fieldShape[0]\n",
    "shape = xdim * ydim * zdim\n",
    "print(\"gridSize (z,y,x, total):\", fieldShape, shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734f1bdb-7922-42d0-a6c4-6a2c4e5132ea",
   "metadata": {},
   "source": [
    "## calculate the fields\n",
    "maybe set the the parallelization mehtod (over fields or particles) manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4b5096-449f-4803-b064-ecf82ca02a93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "if particleCount <= shape:    # maybe add a constant, but needs testing for optimal choise\n",
    "    # calculate every field position in parallel (loop over the particles)\n",
    "    # few particles, many field positions\n",
    "    blockdim = 256                                              # number of threads per block (multiple 32 for optimal speed)\n",
    "    griddim = math.ceil(shape / blockdim)                       # number of blocks in the grid\n",
    "    cudaFunc = FieldParallel\n",
    "    print(\"Fields in parallel\")\n",
    "    \n",
    "else:\n",
    "    # calculate every particle in parallel (loop over the entire field)\n",
    "    # many particles, few field positions\n",
    "    blockdim = 32  #256                                         # number of threads per block (multiple 32 for optimal speed)\n",
    "    griddim = math.ceil(particleCount / blockdim)         # number of blocks in the grid\n",
    "    cudaFunc = particleParallel\n",
    "    print(\"particles in parallel\")\n",
    "    \n",
    "print(\"particles to be processed:\", particleCount)\n",
    "\n",
    "# for test on a smaller number of particles\n",
    "#blockdim = 1\n",
    "#griddim = 1\n",
    "\n",
    "array_dtype = np.float32                            # dtype for arrays\n",
    "ex = np.zeros(shape, dtype=array_dtype)\n",
    "ey = np.zeros(shape, dtype=array_dtype)\n",
    "ez = np.zeros(shape, dtype=array_dtype)\n",
    "bx = np.zeros(shape, dtype=array_dtype)\n",
    "by = np.zeros(shape, dtype=array_dtype)\n",
    "bz = np.zeros(shape, dtype=array_dtype)\n",
    "\n",
    "# All variables with _d at the and are for device variables\n",
    "print(\"allocate/copy field/data arrays to device\")\n",
    "ex_d = cuda.to_device(ex)\n",
    "ey_d = cuda.to_device(ey)\n",
    "ez_d = cuda.to_device(ez)\n",
    "bx_d = cuda.to_device(bx)\n",
    "by_d = cuda.to_device(by)\n",
    "bz_d = cuda.to_device(bz)\n",
    "\n",
    "charge_d = cuda.to_device(charge)\n",
    "xpos_d = cuda.to_device(xpos)\n",
    "ypos_d = cuda.to_device(ypos)\n",
    "zpos_d = cuda.to_device(zpos)\n",
    "momentumx_d = cuda.to_device(momentumx)\n",
    "momentumy_d = cuda.to_device(momentumy)\n",
    "momentumz_d = cuda.to_device(momentumz)\n",
    "mass_d = cuda.to_device(mass)\n",
    "weightings_d = cuda.to_device(weightings)\n",
    "\n",
    "print(\"\\nstart of field calculation time:\", time.ctime())\n",
    "starttime = time.time()\n",
    "\n",
    "#cudaFunc[griddim, blockdim](ex, ey, ez, bx, by, bz, charge, xpos, ypos, zpos, momentumx, momentumy, momentumz, mass, weightings, xdim, ydim, zdim, particleCount)\n",
    "cudaFunc[griddim, blockdim](ex_d, ey_d, ez_d, bx_d, by_d, bz_d, charge_d, xpos_d, ypos_d, zpos_d, momentumx_d, momentumy_d, momentumz_d, mass_d, weightings_d, xdim, ydim, zdim, particleCount)\n",
    "\n",
    "exeTime = time.time()-starttime\n",
    "print(\"time: {:.5}\".format( exeTime ), \"s\")\n",
    "print(\"avgTime per particle per cell:\", exeTime / particleCount / (xdim*ydim*zdim), \"s/(particle*cell)\")\n",
    "\n",
    "## if the kernel dies here, start the jupyter session with more memory\n",
    "#floatSize = 4 # 4 for float32, 8 for float64\n",
    "#print(\"min.\", shape * 6 * floatSize , \"GB for field arrays\")\n",
    "\n",
    "print(\"\\nget field arrays from device\", time.ctime())\n",
    "ex_d.copy_to_host(ex)\n",
    "ey_d.copy_to_host(ey)\n",
    "ez_d.copy_to_host(ez)\n",
    "bx_d.copy_to_host(bx)\n",
    "by_d.copy_to_host(by)\n",
    "bz_d.copy_to_host(bz)\n",
    "print(\"field arrays copied from device to host\", time.ctime())\n",
    "\n",
    "# delete device variables, no longer needed after the parallel computation\n",
    "del charge_d\n",
    "del xpos_d\n",
    "del ypos_d\n",
    "del zpos_d\n",
    "del momentumx_d\n",
    "del momentumy_d\n",
    "del momentumz_d\n",
    "del mass_d\n",
    "del weightings_d\n",
    "\n",
    "if write == True:\n",
    "    smoothFields(ex, ey, ez, bx, by, bz)\n",
    "    \n",
    "    write()\n",
    "    \n",
    "    print(\"\\nwrote to checkpoint\")\n",
    "else:\n",
    "    print(\"\\nno data was written to the checkpoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98ab1a3-a2fd-497a-baba-c9b0a806bc75",
   "metadata": {},
   "source": [
    "## Write fields to the checkpoint manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cb54e4-4ed4-43a9-8244-219d771239f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only smooth if the fields aren't already smoothed in the automatic writing\n",
    "if do_write == False:\n",
    "    print(\"smoothing fields... may take some time\")\n",
    "    smoothFields(ex, ey, ez, bx, by, bz)\n",
    "    print(\"Fields were smoothed\")\n",
    "else:\n",
    "    print(\"nothing happend, already smoothed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88f2d81-6d83-49a9-9ade-de66d5fbaf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual write\n",
    "if do_write == False:    \n",
    "    write()\n",
    "    print(\"data written\")\n",
    "    \n",
    "else:\n",
    "    print(\"nothing happened, data was already written\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e358894-7089-4811-a8d0-5eab7e741b83",
   "metadata": {},
   "source": [
    "# Visualization of the fields if necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de532207-001b-46bb-9127-092d7784c6b6",
   "metadata": {},
   "source": [
    "and other stuff, not realy documented with comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3188b42-1a92-43e7-ae7e-d992bcda51ab",
   "metadata": {},
   "source": [
    "## absolute field values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49779d2f-4d44-4e08-94d4-58ecfa17ef9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# absolute values of the fields\n",
    "ef = np.sqrt(ex**2+ey**2+ez**2)\n",
    "bf = np.sqrt(bx**2+by**2+bz**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cccdc8-d98b-4aab-9f79-31c9d60abf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize(10, 10)\n",
    "depth = 400\n",
    "imshow((ef.reshape(zdim, ydim, xdim)[depth].T))\n",
    "colorbar()\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a219a4-526f-4282-8447-8a01fab1dd48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d129ada-85fc-4a09-a2d0-55f5a80e4f22",
   "metadata": {},
   "source": [
    "## show particle positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca4b9f8-c549-4c05-bbf4-bc88a14ca742",
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize(15,15)\n",
    "a = np.histogram2d(xpos, ypos, weights=weightings, bins=[np.linspace(0, xdim, xdim+1), np.linspace(0, ydim, ydim+1)] )\n",
    "imshow(a[0])\n",
    "xlabel(\"y cells\")\n",
    "ylabel(\"x cells\")\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc588cc-7f91-4c87-a55b-f5f51e6471b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "del ex_d\n",
    "del ey_d\n",
    "del ez_d\n",
    "del bx_d\n",
    "del by_d\n",
    "del bz_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573c2aaf-6245-46c6-bee2-a68b573c6acb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7d86df-8cfe-4534-90ca-65f04dd692f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9492bf31-2cce-4a57-96c4-f58151677718",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96ba856-cda9-47c5-af5b-1fef7b8b474b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
